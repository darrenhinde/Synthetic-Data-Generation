{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Setup Instructions\n",
    "\n",
    "To set up your environment for this workshop, follow these steps:\n",
    "\n",
    "```\n",
    "uv venv --python 3.9 \n",
    "source .venv/bin/activate\n",
    "uv pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Use this kernel when running these notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Setup\n",
    "\n",
    "Now to setup our vector database we will use LanceDB, this is a vector database that is very easy to setup and use.\n",
    "\n",
    "It is open source and can be run locally, or in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should in turn create a `lancedb` directory in your current working directory. We can validate that this is the case by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.exists(\"./lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Creating a Vector Table in LanceDB\n",
    "\n",
    "### Next, we'll create our first vector table by:\n",
    " \n",
    " 1. Defining a Pydantic schema to structure our data\n",
    " 2. Using LanceDB's `Table` class to create and manage the table\n",
    " 3. Integrating with embedding models to vectorize our documents\n",
    "\n",
    " LanceDB supports multiple embedding providers including OpenAI, Cohere, and HuggingFace. For this workshop, we'll use OpenAI's embedding models which offer an excellent balance of quality and performance.\n",
    "\n",
    " You can explore all available embedding models in the [LanceDB documentation](https://lancedb.github.io/lancedb/embeddings/available_embedding_models/text_embedding_functions/).\n",
    "\n",
    " Let's start by defining our table schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "# Define a Schema\n",
    "class Words(LanceModel):\n",
    "    # This is the source field that will be used as input to the OpenAI Embedding API\n",
    "    text: str = func.SourceField()\n",
    "\n",
    "    # This is the vector field that will store the output of the OpenAI Embedding API\n",
    "    vector: Vector(func.ndims()) = func.VectorField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our table with this schema. By using Pydantic, LanceDB will create the necessary fields for us and we can use the `add` method to ingest our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[\u001b[0m2025-03-14T17:49:18Z \u001b[33mWARN \u001b[0m lance::dataset::write::insert\u001b[90m]\u001b[0m No existing dataset at /Users/darrenhinde/Documents/GitHub/synthetic-generation/Workshop/lancedb/words.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "table = db.create_table(\"words\", schema=Words, mode=\"overwrite\")\n",
    "\n",
    "# Ingest our data\n",
    "table.add(\n",
    "    [\n",
    "        {\"text\": \"hello world\"},\n",
    "        {\"text\": \"goodbye world\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello world</td>\n",
       "      <td>[-0.006701672, -0.03921971, 0.034169834, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goodbye world</td>\n",
       "      <td>[0.02580526, -0.0054752463, 0.011662952, 0.012...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text                                             vector\n",
       "0    hello world  [-0.006701672, -0.03921971, 0.034169834, 0.028...\n",
       "1  goodbye world  [0.02580526, -0.0054752463, 0.011662952, 0.012..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full text search\n",
    "\n",
    "Now that we have our table created, we can use the `search` method to search for documents in our table.\n",
    "\n",
    "table.search(\"hello world\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "lance error: Invalid user input: Cannot perform full text search unless an INVERTED index has been created on at least one column, /Users/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lance-0.24.1/src/dataset/scanner.rs:1555:17",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/query.py:334\u001b[0m, in \u001b[0;36mLanceQueryBuilder.to_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_list\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    Execute the query and return the results as a list of dictionaries.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    fields are returned whether or not they're explicitly selected.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pylist()\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/query.py:943\u001b[0m, in \u001b[0;36mLanceFtsQueryBuilder.to_arrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhrase query is not yet supported in Lance FTS. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse tantivy-based index instead for now.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    930\u001b[0m query \u001b[38;5;241m=\u001b[39m Query(\n\u001b[1;32m    931\u001b[0m     columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns,\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_where,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m     offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset,\n\u001b[1;32m    942\u001b[0m )\n\u001b[0;32m--> 943\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m results \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mread_all()\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reranker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/table.py:2233\u001b[0m, in \u001b[0;36mLanceTable._execute_query\u001b[0;34m(self, query, batch_size)\u001b[0m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute_query\u001b[39m(\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: Query, batch_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mRecordBatchReader:\n\u001b[0;32m-> 2233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLOOP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/background_loop.py:25\u001b[0m, in \u001b[0;36mBackgroundEventLoop.run\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_coroutine_threadsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/table.py:3282\u001b[0m, in \u001b[0;36mAsyncTable._execute_query\u001b[0;34m(self, query, batch_size)\u001b[0m\n\u001b[1;32m   3279\u001b[0m     fts_columns \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mfull_text_query\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m   3280\u001b[0m     async_query \u001b[38;5;241m=\u001b[39m async_query\u001b[38;5;241m.\u001b[39mnearest_to_text(fts_query, columns\u001b[38;5;241m=\u001b[39mfts_columns)\n\u001b[0;32m-> 3282\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m async_query\u001b[38;5;241m.\u001b[39mto_arrow()\n\u001b[1;32m   3283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39mto_reader()\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/query.py:1662\u001b[0m, in \u001b[0;36mAsyncQueryBase.to_arrow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_arrow\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;124;03m    Execute the query and collect the results into an Apache Arrow Table.\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m    [to_batches][lancedb.query.AsyncQueryBase.to_batches]\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1662\u001b[0m     batch_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_batches()\n\u001b[1;32m   1663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(\n\u001b[1;32m   1664\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m batch_iter\u001b[38;5;241m.\u001b[39mread_all(), schema\u001b[38;5;241m=\u001b[39mbatch_iter\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m   1665\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/query.py:1992\u001b[0m, in \u001b[0;36mAsyncFTSQuery.to_batches\u001b[0;34m(self, max_batch_length)\u001b[0m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_batches\u001b[39m(\n\u001b[1;32m   1990\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, max_batch_length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncRecordBatchReader:\n\u001b[0;32m-> 1992\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto_batches()\n\u001b[1;32m   1993\u001b[0m     results \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(\u001b[38;5;28;01mawait\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mread_all(), reader\u001b[38;5;241m.\u001b[39mschema)\n\u001b[1;32m   1994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reranker:\n",
      "File \u001b[0;32m~/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/lancedb/query.py:1652\u001b[0m, in \u001b[0;36mAsyncQueryBase.to_batches\u001b[0;34m(self, max_batch_length)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_batches\u001b[39m(\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, max_batch_length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1639\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncRecordBatchReader:\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;124;03m    Execute the query and return the results as an Apache Arrow RecordBatchReader.\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;124;03m        underlying data is stored in smaller chunks.\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncRecordBatchReader(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner\u001b[38;5;241m.\u001b[39mexecute(max_batch_length))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: lance error: Invalid user input: Cannot perform full text search unless an INVERTED index has been created on at least one column, /Users/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/lance-0.24.1/src/dataset/scanner.rs:1555:17"
     ]
    }
   ],
   "source": [
    "table.search(\"hello\",query_type=\"fts\").to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we run the code above, we get an error saying \"inverted index not found\". This happens because full text search requires a special index that connects words to their documents.\n",
    " \n",
    "### What is an Inverted Index?\n",
    "\n",
    "An inverted index is simply a lookup table that shows which documents contain specific words. Think of it like the index at the back of a book that tells you which pages contain certain topics.\n",
    "\n",
    "It connects words to the documents where they appear. We must create this index beforehand to make text searches fast and efficient.\n",
    "\n",
    "Let's look at a simple example to understand this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have an initial document\n",
    "documents = {\n",
    "    1: \"The quick brown fox\",\n",
    "    2: \"The lazy brown dog\",\n",
    "    3: \"The fox jumps over dog\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {\n",
    "    'the': {1, 2, 3},\n",
    "    'quick': {1},\n",
    "    'brown': {1, 2}, \n",
    "    'fox': {1, 3},\n",
    "    'lazy': {2},\n",
    "    'dog': {2, 3},\n",
    "    'jumps': {3},\n",
    "    'over': {3}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that when users make a query like `the dog`, we can quickly look up the documents that contain these words and return the results. \n",
    "\n",
    "We use a simplified implementation here where we check for each word in the query and then return the documents that contain any of the words. A document that has more matches has a higher score and will be ranked higher in the returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: [{'doc_id': 1, 'score': 1.0}, {'doc_id': 2, 'score': 0.5}, {'doc_id': 3, 'score': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "def search(query, inverted_index):\n",
    "    # Convert query to lowercase and split into words\n",
    "    query_words = query.lower().split()\n",
    "    \n",
    "    # Count matches for each document\n",
    "    doc_scores = {}\n",
    "    for word in query_words:\n",
    "        if word in inverted_index:\n",
    "            for doc_id in inverted_index[word]:\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1\n",
    "    \n",
    "    # Sort documents by score in descending order\n",
    "    sorted_results = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return list of (doc_id, score) tuples\n",
    "    return [\n",
    "        {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"score\": score/ len(query_words)\n",
    "        }\n",
    "        for doc_id, score in sorted_results\n",
    "    ]\n",
    "\n",
    "query = \"the quick\"\n",
    "results = search(query, inverted_index)\n",
    "print(f\"Search results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B25 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.create_fts_index(\"text\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "for item in table.search(\"hello\",query_type=\"fts\").to_list():\n",
    "    print(item[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "goodbye world\n"
     ]
    }
   ],
   "source": [
    "for item in table.search(\"hello OR goodbye\",query_type=\"fts\").to_list():\n",
    "    print(item[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Different Retrieval Methods\n",
    "\n",
    "Let's explore the key retrieval methods we've covered and how they complement each other:\n",
    "\n",
    "1. **Vector Search**: Transforms text into numerical vectors that capture semantic meaning. This approach identifies documents with vectors most similar to your query vector, making it excellent for finding conceptually related content regardless of exact wording. For instance, \"I'm delighted\" and \"I'm really happy\" would be recognized as semantically equivalent.\n",
    "\n",
    "2. **Full Text Search**: Performs direct word matching between your query and document content. It employs sophisticated algorithms like BM25 scoring to rank results based on term frequency and uniqueness across the corpus. This method excels at precise keyword matching and finding specific information.\n",
    "\n",
    "3. **Hybrid Search**: Leverages the strengths of both vector and full-text approaches. By combining semantic understanding with exact keyword matching, hybrid search delivers more comprehensive results. The system queries both methods independently and then intelligently merges the results to provide the most relevant documents.\n",
    "\n",
    "Now, let's implement hybrid search in LanceDB to demonstrate its effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Setup\n",
    "\n",
    "We will use the `arize-phoenix` library to log our data and metrics. This is a powerful library that allows us to log data and metrics to a central location, and then view them in a dashboard.\n",
    "\n",
    "\n",
    "You will need to go to https://app.phoenix.arize.com/ and create an account.\n",
    "\n",
    "Once you have an account you will need to create an API key.\n",
    "\n",
    "You can then use this API key to log data and metrics to your account.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix environment variables are configured:\n",
      "Endpoint: https://app.phoenix.arize.com\n",
      "Headers configured: True\n"
     ]
    }
   ],
   "source": [
    "# Let's set up Phoenix for logging and tracing\n",
    "# We'll use the API key from our .env file to authenticate with Phoenix\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if Phoenix environment variables are set\n",
    "phoenix_endpoint = os.environ.get(\"PHOENIX_COLLECTOR_ENDPOINT\")\n",
    "phoenix_headers = os.environ.get(\"PHOENIX_CLIENT_HEADERS\")\n",
    "\n",
    "if phoenix_endpoint and phoenix_headers:\n",
    "    print(f\"Phoenix environment variables are configured:\")\n",
    "    print(f\"Endpoint: {phoenix_endpoint}\")\n",
    "    print(f\"Headers configured: {'api_key' in phoenix_headers}\")\n",
    "else:\n",
    "    print(\"Phoenix environment variables not found. Please check your .env file.\")\n",
    "    # You can set them manually if needed\n",
    "    # os.environ[\"PHOENIX_CLIENT_HEADERS\"] = \"api_key=YOUR_API_KEY\"\n",
    "    # os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: my-llm-app\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenhinde/Documents/GitHub/synthetic-generation/Workshop/.venv/lib/python3.9/site-packages/phoenix/otel/otel.py:530: UserWarning: No OpenInference instrumentors found. Maybe you need to update your OpenInference version? Skipping auto-instrumentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=\"my-llm-app\", # Default is 'default'\n",
    "  auto_instrument=True # See 'Trace all calls made to a library' below\n",
    ")\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Phoenix tracing...\n",
      "Input: 'Hello, world!' â†’ Output: 'Processed: Hello, world!'\n",
      "Input: 'Testing tracing' â†’ Output: 'Processed: Testing tracing'\n",
      "Input: 'Phoenix is awesome' â†’ Output: 'Processed: Phoenix is awesome'\n",
      "\n",
      "Check your Phoenix dashboard to see the traces!\n"
     ]
    }
   ],
   "source": [
    "@tracer.chain\n",
    "def my_func(input: str) -> str:\n",
    "    # This is a simple function that will be traced by Phoenix\n",
    "    processed_result = f\"Processed: {input}\"\n",
    "    return processed_result\n",
    "\n",
    "# Let's test our tracing with a few examples\n",
    "print(\"Testing Phoenix tracing...\")\n",
    "test_inputs = [\"Hello, world!\", \"Testing tracing\", \"Phoenix is awesome\"]\n",
    "for test_input in test_inputs:\n",
    "    result = my_func(test_input)\n",
    "    print(f\"Input: '{test_input}' â†’ Output: '{result}'\")\n",
    "\n",
    "print(\"\\nCheck your Phoenix dashboard to see the traces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
