{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "In `make_synthetic_questions.ipynb`, we generated synthetic questions to bootstrap evaluation of the retrieval system in our hardware store's Q&A system.\n",
    "\n",
    "This notebook shows the first step in calculating precision and recall with different retrieval parameters. We will run more advanced experiments in future notebooks after we have these baseline scores.\n",
    "\n",
    "## Data\n",
    "\n",
    "Here is a brief review of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lancedb\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'COHERE_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[0;32m---> 11\u001b[0m cohere_api_key \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCOHERE_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mEvalQuestion\u001b[39;00m(BaseModel):\n\u001b[1;32m     15\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'COHERE_API_KEY'"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from diskcache import Cache\n",
    "import lancedb\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "cohere_api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "\n",
    "\n",
    "class EvalQuestion(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    chunk_id: str\n",
    "    question_with_context: str\n",
    "\n",
    "\n",
    "def score(hits):\n",
    "    n_retrieval_requests = len(hits)\n",
    "    total_retrievals = sum(len(l) for l in hits)\n",
    "    true_positives = sum(sum(sublist) for sublist in hits)\n",
    "    precision = true_positives / total_retrievals if total_retrievals > 0 else 0\n",
    "    recall = true_positives / n_retrieval_requests if n_retrieval_requests > 0 else 0\n",
    "    return {\"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def run_reranked_request(\n",
    "    q: EvalQuestion,\n",
    "    reviews_table: lancedb.table.LanceTable,\n",
    "    max_n_return_vals: int,\n",
    "    n_to_rerank: int = 40,\n",
    "    model: str = \"rerank-english-v3.0\",\n",
    ") -> List[bool]:\n",
    "    cache = Cache(\"./cohere_cache\")\n",
    "    cache_key = f\"{q.question_with_context}_{max_n_return_vals}_{model}\".replace(\n",
    "        \"?\", \"\"\n",
    "    )\n",
    "\n",
    "    cached_result = cache.get(cache_key)\n",
    "    if cached_result is not None:\n",
    "        return cached_result\n",
    "\n",
    "    initial_results = (\n",
    "        reviews_table.search(q.question_with_context)\n",
    "        .select([\"id\", \"review\"])\n",
    "        .limit(n_to_rerank)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    texts = [r[\"review\"] for r in initial_results]\n",
    "\n",
    "    # Rerank using Cohere\n",
    "    co = cohere.Client(cohere_api_key)\n",
    "    reranked = co.rerank(\n",
    "        query=q.question_with_context,\n",
    "        documents=texts,\n",
    "        top_n=max_n_return_vals,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Map reranked results back to original IDs\n",
    "    reranked_ids = [initial_results[r.index][\"id\"] for r in reranked.results]\n",
    "    result = [str(q.chunk_id) == str(r) for r in reranked_ids]\n",
    "    cache.set(cache_key, result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def score_reranked_search(\n",
    "    eval_questions: List[EvalQuestion],\n",
    "    reviews_table: lancedb.table.LanceTable,\n",
    "    k_values: List[int],\n",
    "    n_to_rerank: int = 40,\n",
    "    model=\"rerank-english-v3.0\",\n",
    ") -> Dict[int, Dict[str, float]]:\n",
    "    max_k = max(k_values)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        all_hits = list(\n",
    "            executor.map(\n",
    "                lambda q: run_reranked_request(\n",
    "                    q, reviews_table, max_k, n_to_rerank, model\n",
    "                ),\n",
    "                eval_questions,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        hits = [h[:k] for h in all_hits]\n",
    "        results[k] = score(hits)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "reviews_table = db.open_table(\"reviews\")\n",
    "reviews_table.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"synthetic_eval_dataset.json\", \"r\") as f:\n",
    "    synthetic_questions = json.load(f)\n",
    "synthetic_questions[:5]\n",
    "eval_questions = [EvalQuestion(**question) for question in synthetic_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Evaluation\n",
    "\n",
    "Load the evaluation questions into a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a simple search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_questions\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_questions' is not defined"
     ]
    }
   ],
   "source": [
    "eval_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_request(q: EvalQuestion, n_return_vals=5):\n",
    "    results = (\n",
    "        reviews_table.search(q.question_with_context).select([\"id\"]).limit(n_return_vals).to_list()\n",
    "    )\n",
    "    return [str(q.chunk_id) == str(r[\"id\"]) for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the benchmarking. For simplicity, we just compare retrieval sizes with a simple semantic search in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>n_retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.116889</td>\n",
       "      <td>0.584444</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095333</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall  n_retrieved\n",
       "0   0.116889  0.584444            5\n",
       "1   0.095333  0.953333           10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_simple_search(n_to_retrieve: List[int]) -> Dict[str, float]:\n",
    "    # parallelize to speed this up 5-10X\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        hits = list(\n",
    "            executor.map(lambda q: run_simple_request(q, n_to_retrieve), eval_questions)\n",
    "        )\n",
    "    return score(hits)\n",
    "\n",
    "k_to_retrieve = [5, 10]\n",
    "scores = pd.DataFrame([score_simple_search(n) for n in k_to_retrieve])\n",
    "scores[\"n_retrieved\"] = k_to_retrieve\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have Cohere set up, you can see uf a reranker improves results (we'll talk more about rerankers in the coming weeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision    recall  n_retrieved\n",
      "0   0.134000  0.670000            5\n",
      "1   0.096667  0.966667           10\n"
     ]
    }
   ],
   "source": [
    "k_to_retrieve = [5, 10]\n",
    "reranked_scores = score_reranked_search(eval_questions, reviews_table, k_to_retrieve)\n",
    "reranked_scores_df = pd.DataFrame([\n",
    "    {\"precision\": scores[\"precision\"], \"recall\": scores[\"recall\"], \"n_retrieved\": k}\n",
    "    for k, scores in reranked_scores.items()\n",
    "])\n",
    "print(reranked_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
